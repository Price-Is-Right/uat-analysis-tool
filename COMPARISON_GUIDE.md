# SIDE-BY-SIDE COMPARISON GUIDE
## Comparing Monolithic vs Microservices Architecture

This guide helps you run and compare both versions of the application side-by-side.

## ðŸ“‹ Quick Start

### Prerequisites
1. **Start all microservices first:**
   ```powershell
   .\start_all_services.ps1
   ```
   Wait 30 seconds for all services to be healthy.

2. **Start both UIs:**
   ```powershell
   .\start_comparison.ps1
   ```

## ðŸ”µ Original App (Monolithic)
- **URL:** http://localhost:5002
- **Architecture:** Flask â†’ Direct Python library imports
- **Pros:**
  - Faster (no network overhead)
  - Simpler deployment (single process)
  - Easier debugging (all code in one place)
- **Cons:**
  - Harder to scale individual components
  - Tight coupling between modules
  - Single point of failure

## ðŸŸ¢ Microservices App (Distributed)
- **URL:** http://localhost:5003
- **Architecture:** Flask â†’ HTTP API â†’ API Gateway â†’ 8 Microservices
- **Services:**
  - API Gateway (8000)
  - Context Analyzer (8001)
  - Search Service (8002)
  - Enhanced Matching (8003)
  - UAT Management (8004)
  - LLM Classifier (8005)
  - Embedding Service (8006)
  - Vector Search (8007)
- **Pros:**
  - Independent scaling per service
  - Loose coupling (easy to modify/replace services)
  - Multiple deployment options
  - Clear service boundaries
- **Cons:**
  - Network overhead (~10-50ms per HTTP call)
  - More complex deployment
  - Requires all services to be running

## ðŸ“Š Comparison Testing

### Test Scenario 1: Basic Analysis
1. Open both http://localhost:5002 and http://localhost:5003
2. Submit the same query: "How do I configure Azure App Service with custom domain?"
3. **Compare:**
   - Response time
   - Analysis results (should be identical)
   - AI reasoning steps
   - Search results

### Test Scenario 2: Context Analysis
1. Submit a complex query with Microsoft products mentioned
2. **Compare:**
   - Detected products (should match)
   - Categories and intents
   - Confidence scores
   - Recommendations

### Test Scenario 3: UAT Search
1. Search for existing UATs
2. **Compare:**
   - Number of results
   - Ranking order
   - Completeness scores
   - Response times

## âš¡ Performance Expectations

### Response Time Comparison
| Operation | Original | Microservices | Difference |
|-----------|----------|---------------|------------|
| Context Analysis | 1-3s | 1.05-3.1s | +50-100ms |
| UAT Search | 50-200ms | 60-220ms | +10-20ms |
| Classification | 1-3s (cached: <10ms) | 1.05-3.1s (cached: <20ms) | +50-100ms |
| Embedding | 1-2s (cached: <10ms) | 1.05-2.1s (cached: <20ms) | +50-100ms |

### Why the Difference?
- **Network Latency:** Each HTTP call adds ~10-20ms
- **Serialization:** JSON encoding/decoding adds ~5-10ms
- **Gateway Routing:** API Gateway adds ~5-10ms
- **Total per service call:** ~20-40ms overhead

### When Does It Matter?
- **Low:** Single queries (user won't notice 100ms)
- **Medium:** Batch operations (100 items = 10 seconds extra)
- **High:** Real-time applications requiring <100ms response

## ðŸ” What to Look For

### âœ… Should Be Identical
- Analysis results
- Detected products/technologies
- Classification categories and intents
- Search result ranking
- Completeness scores
- Recommendations

### âš ï¸ Expected Differences
- Response times (microservices slightly slower)
- Startup time (microservices needs all services)
- Memory usage (microservices uses more - 8 processes)

### âŒ Red Flags (Should Not Happen)
- Different analysis conclusions
- Missing products or technologies
- Different search results order
- Classification mismatches
- Missing recommendations

## ðŸ› Troubleshooting

### Microservices App Returns Errors
1. Check if all microservices are running:
   ```powershell
   Invoke-WebRequest http://localhost:8000/health
   ```
2. Restart microservices if needed:
   ```powershell
   Get-Process python -ErrorAction SilentlyContinue | Stop-Process -Force
   .\start_all_services.ps1
   ```

### Original App Works, Microservices Doesn't
- Check console output for HTTP errors
- Verify API Gateway is routing correctly
- Check service logs for exceptions

### Both Apps Show Different Results
- âš ï¸ This indicates a bug! Please report:
  - Query used
  - Results from both apps
  - Console output from both
  - Service logs from microservices

## ðŸ“ˆ Scalability Demonstration

### Original App
To handle more load:
- âŒ Can't scale individual AI components
- âœ… Can only scale entire application
- Result: Inefficient resource usage

### Microservices App
To handle more load:
- âœ… Scale only the bottleneck service
- âœ… Example: 3 instances of Embedding Service (8006)
- âœ… Example: 2 instances of Context Analyzer (8001)
- Result: Efficient resource allocation

## ðŸŽ¯ Conclusion

**Use Original App If:**
- You want fastest response times
- Simple deployment is priority
- Small scale (<100 requests/hour)
- Development/testing only

**Use Microservices App If:**
- You need independent scaling
- High availability is required
- Multiple teams working on different services
- Production deployment with scaling needs
- Container/Kubernetes deployment

## ðŸ“š Related Documentation
- [ARCHITECTURE.md](ARCHITECTURE.md) - System architecture details
- [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md) - Deployment instructions
- [PROJECT_FINAL_SUMMARY.md](PROJECT_FINAL_SUMMARY.md) - Project overview

## ðŸ”— Port Reference
| Service | Port | Description |
|---------|------|-------------|
| Original App | 5002 | Monolithic Flask app |
| Microservices App | 5003 | Distributed Flask app |
| API Gateway | 8000 | Central routing |
| Context Analyzer | 8001 | AI analysis |
| Search Service | 8002 | UAT search |
| Enhanced Matching | 8003 | Multi-source matching |
| UAT Management | 8004 | CRUD operations |
| LLM Classifier | 8005 | Classification |
| Embedding Service | 8006 | Vector embeddings |
| Vector Search | 8007 | Semantic search |
