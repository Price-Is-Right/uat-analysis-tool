# AI Configuration Environment Variables

# =============================================================================
# AZURE OPENAI CONFIGURATION
# =============================================================================

# Azure OpenAI endpoint (required)
# Example: https://your-resource-name.openai.azure.com/
AZURE_OPENAI_ENDPOINT=

# Azure OpenAI API key (required)
# Get from Azure Portal: Azure OpenAI resource > Keys and Endpoint
AZURE_OPENAI_API_KEY=

# Deployment names (optional - defaults to model names)
# These should match your Azure OpenAI deployment names
AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-large
AZURE_OPENAI_CLASSIFICATION_DEPLOYMENT=gpt-4o

# =============================================================================
# AZURE DEVOPS CONFIGURATION (existing)
# =============================================================================

# Azure DevOps organization
ADO_ORGANIZATION=

# Azure DevOps project
ADO_PROJECT=

# Azure DevOps PAT token (optional - prefer Azure CLI authentication)
# Only set this for automated/production scenarios
# For development, use: az login
ADO_PAT=


# =============================================================================
# SETUP INSTRUCTIONS
# =============================================================================

# 1. Copy this file to .env (ignored by git):
#    cp .env.template .env

# 2. Fill in your Azure OpenAI credentials:
#    - Get endpoint and key from Azure Portal
#    - Create deployments for text-embedding-3-large and gpt-4o
#    - Update deployment names if different from defaults

# 3. For Azure DevOps (optional):
#    - Development: Run "az login" (no PAT needed)
#    - Production: Create service principal and set credentials

# 4. Test configuration:
#    python ai_config.py

# =============================================================================
# COST MANAGEMENT
# =============================================================================

# Cost optimization is handled automatically through:
# - 7-day cache TTL (reduces API calls)
# - API-first strategy (freshest data when possible)
# - Intelligent cache fallback (when API slow)
# - Pattern matching features (reduces LLM reliance)

# Expected costs (approximate):
# - Embeddings: ~$0.00013 per 1K tokens (text-embedding-3-large)
# - Classification: ~$0.01 per 1K tokens (GPT-4o input/output)
# - Cache hit rate: 60-80% after warm-up
# - Estimated monthly cost: $50-200 for typical usage

# =============================================================================
# FINE-TUNING (FUTURE)
# =============================================================================

# After collecting sufficient corrections:
# 1. Run: python prepare_finetuning.py
# 2. Upload training files to Azure OpenAI
# 3. Create fine-tuning job
# 4. Update AZURE_OPENAI_CLASSIFICATION_DEPLOYMENT to fine-tuned model
